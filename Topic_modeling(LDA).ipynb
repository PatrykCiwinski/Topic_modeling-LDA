{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1bxPjFe_A0ICWNTNZNP4IbptHD1b71Ynq",
      "authorship_tag": "ABX9TyMt3rt4ydUtLLk2w0G40ZXa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PatrykCiwinski/Topic_modeling-LDA/blob/main/Topic_modeling(LDA).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LDA"
      ],
      "metadata": {
        "id": "2XKrLXi0Yw0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "PrZtzNbNatR2"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1iboAP6jwWe0FnhYlz97Q_62Udc2aTNp4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82waVdS2ckJU",
        "outputId": "04745761-1f07-4647-88de-10a3bbeea2ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1iboAP6jwWe0FnhYlz97Q_62Udc2aTNp4\n",
            "To: /content/bbc_text_cls.csv\n",
            "\r  0% 0.00/5.09M [00:00<?, ?B/s]\r100% 5.09M/5.09M [00:00<00:00, 151MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('bbc_text_cls.csv')"
      ],
      "metadata": {
        "id": "FeU6ybOCarKe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3WGUnYznud2",
        "outputId": "1917538f-0dd8-4ff2-9040-8b3c7026afcc"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stop-words\n",
        "stop_words="
      ],
      "metadata": {
        "id": "6PeetvjPm313"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3PFbApaoFRu",
        "outputId": "484ad970-7a9a-4d00-f26c-5a4a8b38bda7"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6TK9x3JoLlw",
        "outputId": "f5047485-5bb4-4893-937b-630a9262c5dc"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(headline):\n",
        "  le=WordNetLemmatizer()\n",
        "  word_tokens=word_tokenize(headline)\n",
        "  tokens=[le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w)>3]\n",
        "  cleaned_text=\" \".join(tokens)\n",
        "  return cleaned_text"
      ],
      "metadata": {
        "id": "Mwa8KX9GmhQK"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_text']=df['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "QZ8182TuixL6"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features=1000)"
      ],
      "metadata": {
        "id": "s2661TZqgF6W"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=vectorizer.fit_transform(df['cleaned_text'])"
      ],
      "metadata": {
        "id": "wCW5JsvSdcAF"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_topics=df['labels'].nunique()"
      ],
      "metadata": {
        "id": "_tSN8sGegb-q"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda=LatentDirichletAllocation(n_components=no_topics)"
      ],
      "metadata": {
        "id": "eXDFQoKndkpy"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = lda.fit_transform(X)"
      ],
      "metadata": {
        "id": "5NIgM_wlds0J"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = lda.components_"
      ],
      "metadata": {
        "id": "_Xv9FMv9d98G"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the topics with their terms\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "for index, component in enumerate(topics):\n",
        "    zipped = zip(terms, component)\n",
        "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]\n",
        "    top_terms_list=list(dict(top_terms_key).keys())\n",
        "    print(\"Topic \"+str(index+1)+\": \",top_terms_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b05-UpPhESK",
        "outputId": "e9de5833-57a4-4503-ca43-f2e7f22eb8dd"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1:  ['said', 'company', 'firm', 'market', 'year', 'bank', 'price']\n",
            "Topic 2:  ['film', 'award', 'best', 'star', 'show', 'year', 'band']\n",
            "Topic 3:  ['mobile', 'people', 'phone', 'technology', 'said', 'game', 'computer']\n",
            "Topic 4:  ['game', 'player', 'club', 'england', 'match', 'team', 'injury']\n",
            "Topic 5:  ['said', 'labour', 'election', 'party', 'blair', 'government', 'would']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We can forecast topics as follows:\n",
        "\n",
        "Topic 1:business,\n",
        "Topic 2:entertainment,\n",
        "Topic 3:tech,\n",
        "Topic 4:sport,\n",
        "Topic 5:politics"
      ],
      "metadata": {
        "id": "fsbgr40lr1lZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7La8_pu3kEb6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}